{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyNet(nn.Module):\n",
    "    def __init__(self, num_in, num_hidden_1, num_hidden_2, num_out):\n",
    "        super(ClassifyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_in,num_hidden_1)\n",
    "        self.dropout1 = nn.Dropout(0.33)\n",
    "        self.fc2 = nn.Linear(num_hidden_1,num_hidden_2)\n",
    "        self.dropout2 = nn.Dropout(0.33)\n",
    "        self.fc3 = nn.Linear(num_hidden_2, num_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using  cuda\n"
     ]
    }
   ],
   "source": [
    "num_hidden_1 = 1024\n",
    "num_hidden_2 = 512\n",
    "batch_size = 1000\n",
    "\n",
    "#GPU\n",
    "cuda = True\n",
    "use_cuda = cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print('Using ',device)\n",
    "\n",
    "#Load Data from CSV and train test split\n",
    "dataset = pd.read_csv('CleanedData _pytorch.csv')\n",
    "dataset = np.array(dataset)\n",
    "\n",
    "data = dataset[:, :-1]\n",
    "target = dataset[:, -1]\n",
    "\n",
    "#Training data (select at random from first 50000)\n",
    "data_size = len(data) - 1\n",
    "data_ind = np.random.permutation(data_size)[:data_size]\n",
    "\n",
    "#Convert data to torch and device\n",
    "data_train = torch.from_numpy(data[data_ind[:50000]]).float().to(device)\n",
    "target_train = torch.from_numpy(target[data_ind[:50000]]).long().to(device)\n",
    "data_test = torch.from_numpy(data[data_ind[50000:]]).float().to(device)\n",
    "target_test = torch.from_numpy(target[data_ind[50000:]]).long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup model and optimizer\n",
    "model = ClassifyNet(42,num_hidden_1,num_hidden_2,3).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)  #Learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration,Testing Accuracy,Training Accuracy\n",
      "0 12.5213623046875 12.603999674320221\n",
      "1 67.64270067214966 67.57400035858154\n",
      "2 73.65273237228394 73.89799952507019\n",
      "3 77.52079367637634 78.1819999217987\n",
      "4 78.53480577468872 79.02799844741821\n",
      "5 79.11587357521057 80.07000088691711\n",
      "6 80.06722331047058 81.44399523735046\n",
      "7 80.33496737480164 81.65199756622314\n",
      "8 80.61410784721375 82.03199505805969\n",
      "9 81.13250732421875 82.88799524307251\n",
      "10 81.26922845840454 82.98799991607666\n",
      "11 81.34897947311401 83.46399664878845\n",
      "12 81.65660500526428 83.91799926757812\n",
      "13 81.75914287567139 84.1539978981018\n",
      "14 82.03258514404297 84.41599607467651\n",
      "15 81.97562098503113 84.83999967575073\n",
      "16 82.21487998962402 84.95999574661255\n",
      "17 82.11803436279297 85.25199890136719\n",
      "18 82.18070268630981 85.56599617004395\n",
      "19 82.18070268630981 85.70199608802795\n",
      "20 82.20348954200745 85.61399579048157\n",
      "21 82.32311606407166 85.90399622917175\n",
      "22 82.61365294456482 86.2779974937439\n",
      "23 82.92127251625061 86.63599491119385\n",
      "24 82.4541449546814 86.84599995613098\n",
      "25 82.79594779014587 86.86800003051758\n",
      "26 82.91557431221008 87.08999752998352\n",
      "27 82.91557431221008 87.49399781227112\n",
      "28 83.05229544639587 87.51199841499329\n",
      "29 82.75037407875061 87.61999607086182\n",
      "30 83.1263542175293 88.09399604797363\n",
      "31 83.10926556587219 88.05399537086487\n",
      "32 83.09217095375061 88.11399936676025\n",
      "33 82.93266892433167 88.4939968585968\n",
      "34 82.57946968078613 88.73399496078491\n",
      "35 82.94975757598877 88.69999647140503\n",
      "36 83.13205242156982 88.99399638175964\n",
      "37 83.02950859069824 89.28399682044983\n",
      "38 83.12065601348877 89.30799961090088\n",
      "39 83.22319984436035 89.5579993724823\n",
      "40 82.75037407875061 89.49999809265137\n",
      "41 83.36561322212219 89.819997549057\n",
      "42 83.2972526550293 90.07200002670288\n",
      "43 83.20041298866272 90.04600048065186\n",
      "44 83.26877355575562 90.28199911117554\n",
      "45 83.38840007781982 90.42999744415283\n",
      "46 83.37131142616272 90.55599570274353\n",
      "47 83.34852457046509 90.73599576950073\n",
      "48 83.39979648590088 90.74199795722961\n",
      "49 83.67893695831299 91.0819947719574\n",
      "50 83.66754055023193 91.27399921417236\n",
      "51 83.39409828186035 91.07999801635742\n",
      "52 83.34852457046509 91.2939965724945\n",
      "53 83.51942896842957 91.34999513626099\n",
      "54 83.2118034362793 91.2880003452301\n",
      "55 83.44537019729614 91.61799550056458\n",
      "56 83.2972526550293 91.84199571609497\n",
      "57 83.37131142616272 92.17000007629395\n",
      "58 83.70172381401062 92.04599857330322\n",
      "59 83.28016400337219 92.0699954032898\n",
      "60 83.5763931274414 92.39400029182434\n",
      "61 83.56500267982483 92.62399673461914\n",
      "62 83.39409828186035 92.44399666786194\n",
      "63 83.65615010261536 92.86999702453613\n",
      "64 83.51373076438904 92.76599884033203\n",
      "65 83.25167894363403 92.87399649620056\n",
      "66 83.45676064491272 93.05199980735779\n",
      "67 83.45676064491272 93.1439995765686\n",
      "68 83.67323875427246 93.36199760437012\n",
      "69 83.7985634803772 93.19999814033508\n",
      "70 83.7985634803772 93.45999956130981\n",
      "71 83.38270783424377 93.65399479866028\n",
      "72 83.57069492340088 93.89599561691284\n",
      "73 83.51942896842957 93.75\n",
      "74 83.34282636642456 94.27199959754944\n",
      "75 83.1434428691864 94.2900002002716\n",
      "76 83.16622972488403 93.89999508857727\n",
      "77 83.38270783424377 94.29199695587158\n",
      "78 83.20041298866272 94.43399906158447\n",
      "79 83.50233435630798 94.42799687385559\n",
      "80 83.57069492340088 94.37999725341797\n",
      "81 83.4909439086914 94.58799958229065\n",
      "82 83.5763931274414 94.80599761009216\n",
      "83 83.37700963020325 94.69799995422363\n",
      "84 83.24598670005798 95.02599835395813\n",
      "85 83.35422277450562 94.9899971485138\n",
      "86 83.53081941604614 95.01999616622925\n",
      "87 83.78717303276062 95.19199728965759\n",
      "88 83.43397378921509 94.97999548912048\n",
      "89 83.34282636642456 95.28599977493286\n",
      "90 83.38840007781982 95.24799585342407\n",
      "91 83.57069492340088 95.39799690246582\n",
      "92 83.39409828186035 95.54799795150757\n",
      "93 83.34852457046509 95.65199613571167\n",
      "94 83.49664211273193 95.4479992389679\n",
      "95 83.26877355575562 95.90199589729309\n",
      "96 83.43967199325562 95.97799777984619\n",
      "97 83.52512121200562 95.91400027275085\n",
      "98 83.37131142616272 95.89599967002869\n",
      "99 83.49664211273193 95.99999785423279\n",
      "100 83.35422277450562 95.98999619483948\n",
      "101 83.41118693351746 96.07399702072144\n",
      "102 83.17192792892456 96.34999632835388\n",
      "103 83.53081941604614 96.32999897003174\n",
      "104 83.53081941604614 96.25200033187866\n",
      "105 83.19471478462219 96.15399837493896\n",
      "106 83.4909439086914 96.41599655151367\n",
      "107 83.2972526550293 96.51999473571777\n",
      "108 83.36561322212219 96.35399580001831\n",
      "109 83.32573771476746 96.48999571800232\n",
      "110 83.10356736183167 96.38599753379822\n",
      "111 83.23459029197693 96.63199782371521\n",
      "112 83.42258334159851 96.58199548721313\n",
      "113 83.43967199325562 96.63999676704407\n",
      "114 83.32573771476746 96.79200053215027\n",
      "115 83.29156041145325 96.8239963054657\n",
      "116 83.27446579933167 96.86799645423889\n",
      "117 83.17762613296509 96.8459963798523\n",
      "118 83.44537019729614 96.9319999217987\n",
      "119 83.5763931274414 97.04999923706055\n",
      "120 83.30864906311035 97.14399576187134\n",
      "121 83.52512121200562 97.08199501037598\n",
      "122 83.32003951072693 97.08399772644043\n",
      "123 83.24598670005798 97.22399711608887\n",
      "124 83.38270783424377 97.3039984703064\n",
      "125 83.19471478462219 97.12799787521362\n",
      "126 83.41688513755798 97.36799597740173\n",
      "127 83.33713412284851 97.32799530029297\n",
      "128 83.30864906311035 97.4020004272461\n",
      "129 83.39979648590088 97.41799831390381\n",
      "130 83.43397378921509 97.3800003528595\n",
      "131 83.26307535171509 97.48599529266357\n",
      "132 83.38840007781982 97.46599793434143\n",
      "133 83.55360627174377 97.61999845504761\n",
      "134 83.25737714767456 97.59399890899658\n",
      "135 83.24028849601746 97.56999611854553\n",
      "136 83.44537019729614 97.52999544143677\n",
      "137 83.27446579933167 97.69399762153625\n",
      "138 83.5422158241272 97.61199951171875\n",
      "139 83.23459029197693 97.67599701881409\n",
      "140 83.1263542175293 97.76399731636047\n",
      "141 83.31434726715088 97.84199595451355\n",
      "142 83.24028849601746 97.75799512863159\n",
      "143 83.22319984436035 97.92400002479553\n",
      "144 83.42828154563904 97.78599739074707\n",
      "145 83.34282636642456 97.86199927330017\n",
      "146 83.4054946899414 97.80599474906921\n",
      "147 83.0409049987793 97.9379951953888\n",
      "148 83.24028849601746 97.92599678039551\n",
      "149 83.2288920879364 97.93199896812439\n",
      "150 83.18331837654114 97.97399640083313\n",
      "151 83.44537019729614 98.05600047111511\n",
      "152 83.37700963020325 97.92199730873108\n",
      "153 83.20610523223877 98.04399609565735\n",
      "154 83.37131142616272 98.20599555969238\n",
      "155 83.23459029197693 98.0519950389862\n",
      "156 83.1434428691864 98.16599488258362\n",
      "157 83.37131142616272 98.13999533653259\n",
      "158 83.2972526550293 98.16599488258362\n",
      "159 83.25167894363403 98.14399480819702\n",
      "160 83.32573771476746 98.20799827575684\n",
      "161 83.19471478462219 98.27999472618103\n",
      "162 83.36561322212219 98.24399948120117\n",
      "163 83.17192792892456 98.20599555969238\n",
      "164 83.11495780944824 98.43199849128723\n",
      "165 83.34282636642456 98.36399555206299\n",
      "166 83.50233435630798 98.26200008392334\n",
      "167 83.1605315208435 98.35999608039856\n",
      "168 83.43397378921509 98.43599796295166\n",
      "169 83.33713412284851 98.33199977874756\n",
      "170 83.39979648590088 98.52599501609802\n",
      "171 83.17192792892456 98.47999811172485\n",
      "172 82.97823667526245 98.53000044822693\n",
      "173 83.38270783424377 98.45199584960938\n",
      "174 83.37700963020325 98.30199480056763\n",
      "175 83.42258334159851 98.44799637794495\n",
      "176 83.34852457046509 98.49199652671814\n",
      "177 83.27446579933167 98.51999878883362\n",
      "178 83.31434726715088 98.56999516487122\n",
      "179 83.24028849601746 98.55799674987793\n",
      "180 83.28016400337219 98.58399629592896\n",
      "181 83.39409828186035 98.58599901199341\n",
      "182 83.34282636642456 98.64599704742432\n",
      "183 83.2972526550293 98.49799871444702\n",
      "184 83.18901658058167 98.75199794769287\n",
      "185 83.10926556587219 98.65599870681763\n",
      "186 83.18331837654114 98.59199523925781\n",
      "187 83.17762613296509 98.7119972705841\n",
      "188 83.29156041145325 98.75199794769287\n",
      "189 83.20610523223877 98.66799712181091\n",
      "190 83.28586220741272 98.7019956111908\n",
      "191 83.35992097854614 98.76199960708618\n",
      "192 83.21750164031982 98.78999590873718\n",
      "193 82.97823667526245 98.76199960708618\n",
      "194 83.05229544639587 98.7339973449707\n",
      "195 82.92697072029114 98.78399968147278\n",
      "196 83.05229544639587 98.7119972705841\n",
      "197 83.19471478462219 98.7879991531372\n",
      "198 83.2972526550293 98.7879991531372\n",
      "199 83.11495780944824 98.69399666786194\n",
      "200 83.42258334159851 98.80799651145935\n",
      "201 83.4738552570343 98.79599809646606\n",
      "202 83.11495780944824 98.83399605751038\n",
      "203 82.93836116790771 98.92999529838562\n",
      "204 83.28016400337219 98.84799718856812\n",
      "205 83.2118034362793 98.91199469566345\n",
      "206 83.58209133148193 98.88399839401245\n",
      "207 83.55360627174377 98.89799952507019\n",
      "208 83.14914107322693 98.99599552154541\n",
      "209 83.44537019729614 98.9359974861145\n",
      "210 83.27446579933167 98.99599552154541\n",
      "211 83.26877355575562 98.96999597549438\n",
      "212 83.28016400337219 98.87999892234802\n",
      "213 83.26877355575562 99.02999997138977\n",
      "214 83.21750164031982 99.0399956703186\n",
      "215 83.37131142616272 98.989999294281\n",
      "216 83.15483927726746 98.95599484443665\n",
      "217 83.17192792892456 98.9579975605011\n",
      "218 83.0750823020935 99.09600019454956\n",
      "219 83.17192792892456 99.01399612426758\n",
      "220 83.2118034362793 99.02599453926086\n",
      "221 83.24598670005798 99.04999732971191\n",
      "222 82.96684622764587 99.00199770927429\n",
      "223 83.32003951072693 99.11399483680725\n",
      "224 83.20610523223877 99.18599724769592\n",
      "225 83.13774466514587 99.10799860954285\n",
      "226 83.14914107322693 99.11999702453613\n",
      "227 83.26307535171509 99.14799928665161\n",
      "228 83.28586220741272 99.08999800682068\n",
      "229 83.23459029197693 99.07599687576294\n",
      "230 83.03520679473877 99.09600019454956\n",
      "231 83.12065601348877 99.25999641418457\n",
      "232 83.09786915779114 99.17599558830261\n",
      "233 83.20610523223877 99.12799596786499\n",
      "234 83.33143591880798 99.15599822998047\n",
      "235 83.01811814308167 99.15599822998047\n",
      "236 83.25167894363403 99.14799928665161\n",
      "237 83.32573771476746 99.16200041770935\n",
      "238 83.09786915779114 99.14199709892273\n",
      "239 83.24598670005798 99.21000003814697\n",
      "240 83.37700963020325 99.22999739646912\n",
      "241 83.34282636642456 99.15599822998047\n",
      "242 83.28586220741272 99.1919994354248\n",
      "243 83.1434428691864 99.2959976196289\n",
      "244 83.10926556587219 99.27799701690674\n",
      "245 83.17762613296509 99.29999709129333\n",
      "246 83.25737714767456 99.26599860191345\n",
      "247 83.39979648590088 99.17199611663818\n",
      "248 83.34282636642456 99.2579996585846\n",
      "249 83.00102353096008 99.20399785041809\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "print('Iteration,Testing Accuracy,Training Accuracy')\n",
    "for i in range(250):\n",
    "\n",
    "    #Model evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = torch.argmax(model(data_test),axis=1)\n",
    "        test_accuracy = torch.sum(pred == target_test)/len(pred)\n",
    "        pred = torch.argmax(model(data_train),axis=1)\n",
    "        train_accuracy = torch.sum(pred == target_train)/len(pred)\n",
    "        print(i,test_accuracy.item()*100,train_accuracy.item()*100)\n",
    "\n",
    "    #Training mode, run data through neural network in mini-batches (SGD)\n",
    "    model.train()\n",
    "    for j in range(0,len(target_train),batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.nll_loss(model(data_train[j:j+batch_size,:]), target_train[j:j+batch_size])\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 0, 1, 0, 0, 2, 0, 0, 1, 1, 1, 0, 0, 0, 0, 2, 0, 0],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(model(data_test[:20]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_test[:20]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
